{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "274b25e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(suppress = True)\n",
    "NUM_ACTIONS: int = 3\n",
    "ROCK, PAPER, SCISSORS = 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "ab1b054a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the adjusted strategy after an iteration\n",
    "def getStrategy(regretSum,strategySum):\n",
    "    normalizingSum = 0\n",
    "    strategy = [0,0,0]\n",
    "    #Normalizingsum is the sum of positive regrets. \n",
    "    #This ensures do not 'over-adjust' and converge to equilibrium\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        if regretSum[i] > 0:\n",
    "            strategy[i] = regretSum[i]\n",
    "        else:\n",
    "            strategy[i] = 0\n",
    "        normalizingSum += strategy[i]\n",
    "    ##This loop normalizes our updated strategy\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        if normalizingSum > 0:\n",
    "            strategy[i] /= normalizingSum\n",
    "        else:\n",
    "            #Default to 33%\n",
    "            strategy[i] = 1.0 / NUM_ACTIONS\n",
    "        strategySum[i] += strategy[i]\n",
    "    return (strategy,strategySum)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "35e696aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAction(strategy):\n",
    "    # Returns a random number in (0,1)\n",
    "    rr = random.random()\n",
    "    # Returns 0, 1, 2 based on the mixed strategy\n",
    "    return np.searchsorted(np.cumsum(strategy/np.sum(strategy)), rr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b7d35daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iters, regretSum, oppStrategy):\n",
    "    actionUtility = np.zeros(NUM_ACTIONS)\n",
    "    strategySum = np.zeros(NUM_ACTIONS)\n",
    "    \n",
    "    for i in range(0,iters):\n",
    "        # Retrieve Actions\n",
    "        t = getStrategy(regretSum,strategySum)\n",
    "        strategy = t[0]\n",
    "        strategySum = t[1]\n",
    "        myAction = getAction(strategy)\n",
    "        # Define an arbitrary opponent strategy from which to adjust\n",
    "        oppAction = getAction(oppStrategy)   \n",
    "        # Opponent Chooses scissors\n",
    "        if oppAction == SCISSORS:\n",
    "            actionUtility[ROCK] = 1\n",
    "            actionUtility[PAPER] = -1\n",
    "        # Opponent Chooses Rock\n",
    "        elif oppAction == ROCK:\n",
    "            actionUtility[SCISSORS] = -1\n",
    "            actionUtility[PAPER] = 1\n",
    "        # Opopnent Chooses Paper\n",
    "        else:\n",
    "            actionUtility[ROCK] = -1\n",
    "            actionUtility[SCISSORS] = 1\n",
    "            \n",
    "        # Add the regrets from this decision\n",
    "        for i in range(NUM_ACTIONS):\n",
    "            regretSum[i] += actionUtility[i] - actionUtility[myAction]\n",
    "            \n",
    "    return strategySum\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a49a47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageStrategy(iters, oppStrat):\n",
    "    strategySum = train(iters,np.zeros(NUM_ACTIONS),oppStrat)\n",
    "    avgStrategy = np.zeros(NUM_ACTIONS)\n",
    "    normalizingSum = 0\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        normalizingSum += strategySum[i]\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        if normalizingSum > 0:\n",
    "            avgStrategy[i] = strategySum[i] / normalizingSum\n",
    "        else:\n",
    "            avgStrategy[i] = 1.0 / NUM_ACTIONS\n",
    "    return avgStrategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1b8845e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValue(p1, p2):\n",
    "    if p1 == p2:\n",
    "        return 0 \n",
    "    if p1 == ROCK and p2 == SCISSORS:\n",
    "        return 1\n",
    "    if p1 == SCISSORS and p2 == PAPER:\n",
    "        return 1\n",
    "    if p1 == PAPER and p2 == ROCK:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def getStrategyPayoff(myStrat, oppStrat, iter1, iter2):\n",
    "    vvv = []\n",
    "    for i in range(iter1):\n",
    "        vv = 0 \n",
    "        for j in range(iter2):\n",
    "            myAction = getAction(myStrat)\n",
    "            oppAction = getAction(oppStrat)\n",
    "            vv += getValue(myAction, oppAction)\n",
    "        vvv.append(vv)\n",
    "\n",
    "    return np.mean(vvv)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0e8725d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opponent's Strategy:  [0.4, 0.3, 0.3]\n",
      "Maximally Exploitative Strat:  [0.00093978 0.99902108 0.00003914]\n",
      "Our Payoff:  10.35\n"
     ]
    }
   ],
   "source": [
    "oppStrat = [0.4,0.3,0.3]\n",
    "myNewStrat = getAverageStrategy(100000, oppStrat)\n",
    "print(\"Opponent's Strategy: \",oppStrat)\n",
    "print(\"Maximally Exploitative Strat: \", myNewStrat)\n",
    "print(\"Our Payoff: \", getStrategyPayoff(myNewStrat, oppStrat, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3b3f9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two player training Function\n",
    "def train2Player(iterations,regretSum1,regretSum2,p2Strat):\n",
    "    ##Adapt Train Function for two players\n",
    "    actionUtility = [0,0,0]\n",
    "    strategySum1 = [0,0,0]\n",
    "    strategySum2 = [0,0,0]\n",
    "    for i in range(0,iterations):\n",
    "        ##Retrieve Actions\n",
    "        t1 = getStrategy(regretSum1,strategySum1)\n",
    "        strategy1 = t1[0]\n",
    "        strategySum1 = t1[1]\n",
    "        myAction = getAction(strategy1)\n",
    "        \n",
    "        t2 = getStrategy(regretSum2,p2Strat)\n",
    "        strategy2 = t2[0]\n",
    "        strategySum2 = t2[1]\n",
    "        oppAction = getAction(strategy2)\n",
    "        \n",
    "        # Opponent Chooses scissors\n",
    "        if oppAction == SCISSORS:\n",
    "            actionUtility[ROCK] = 1\n",
    "            actionUtility[PAPER] = -1\n",
    "        # Opponent Chooses Rock\n",
    "        elif oppAction == ROCK:\n",
    "            actionUtility[SCISSORS] = -1\n",
    "            actionUtility[PAPER] = 1\n",
    "        # Opopnent Chooses Paper\n",
    "        else:\n",
    "            actionUtility[ROCK] = -1\n",
    "            actionUtility[SCISSORS] = 1\n",
    "            \n",
    "        # Add the regrets from this decision\n",
    "        for i in range(NUM_ACTIONS):\n",
    "            regretSum1[i] += actionUtility[i] - actionUtility[myAction]\n",
    "            regretSum2[i] += -(actionUtility[i] - actionUtility[myAction])\n",
    "    return (strategySum1, strategySum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6b876d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the Mixed Nash Equilibrium reached by two opponents through Regret Matching\n",
    "def convergeToNash(iters,oppStrat):\n",
    "    strats = train2Player(iters,[0,0,0],[0,0,0],oppStrat)\n",
    "    s1 = np.sum(strats[0])\n",
    "    s2 = np.sum(strats[1])\n",
    "    for i in range(NUM_ACTIONS):\n",
    "        if s1 > 0:\n",
    "            strats[0][i] = strats[0][i]/s1\n",
    "        if s2 > 0:\n",
    "            strats[1][i] = strats[1][i]/s2\n",
    "    return strats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "60a893e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed Strategy used by Player 1: [0.3403435771409826, 0.3240245448493433, 0.33563187800967414]\n",
      "Mixed Strategy used by Player 2: [0.3249888438137622, 0.34307156412335443, 0.3319395920628834]\n",
      "Payoff of Player 1:  -1.73\n",
      "Payoff of Player 2  1.73\n"
     ]
    }
   ],
   "source": [
    "nashEq = convergeToNash(100000, [0.4,0.3,0.3])\n",
    "payoffP1 = getStrategyPayoff(nashEq[0], nashEq[1], 100, 100)\n",
    "\n",
    "print(\"Mixed Strategy used by Player 1:\", nashEq[0])\n",
    "print(\"Mixed Strategy used by Player 2:\", nashEq[1])\n",
    "print(\"Payoff of Player 1: \", payoffP1)\n",
    "print(\"Payoff of Player 2 \", -payoffP1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
